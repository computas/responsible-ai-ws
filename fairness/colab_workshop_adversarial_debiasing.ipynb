{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "colab_workshop_adversarial_debiasing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/computas/responsible-ai-ws/blob/master/fairness/colab_workshop_adversarial_debiasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_NepKWve3LD",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial debiasing algorithm to learn a fair classifier\n",
        "\n",
        "This is the second notebook from our [tutorial on fair ML pipelines](https://github.com/computas/responsible-ai-ws/blob/master/fairness/workshop.md). This notebook is an adaptataion of [demo_adversarial_debiasing.ipynb](https://github.com/IBM/AIF360/blob/master/examples/demo_adversarial_debiasing.ipynb) from [AIF360's repo](https://github.com/IBM/AIF360). \n",
        "\n",
        "We have made some modifications in order to run the notebook in Google colab. You can also run the orginal version on-premise, by creating a virutal environment as detailed [here](https://github.com/IBM/AIF360#optional-create-a-virtual-environment).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3IYz9Vcd46z",
        "colab_type": "text"
      },
      "source": [
        "## Colab instructions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5R8DDwIeDyz",
        "colab_type": "code",
        "outputId": "3c9d1a8c-0803-4813-c79b-8058a7dc27f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Verify that the environment has python 3\n",
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyUy_rRXfNLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ab61600-c693-41c7-cc31-3f7bc9a7e2ea"
      },
      "source": [
        "# This notebook runs in Tensorflow 1.x. We need this magic command to tell Colab about that.\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QCw5PcVdE7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U \\\n",
        "  aif360==0.2.3 \\\n",
        "  tqdm==4.46.0 \\\n",
        "  numpy==1.18.4 \\\n",
        "  matplotlib==3.2.1 \\\n",
        "  pandas==1.0.3 \\\n",
        "  scipy==1.4.1 \\\n",
        "  \"cvxpy>=1.0\" \\\n",
        "  scs==2.1.2 \\\n",
        "  \"numba>=0.42.0\" \\\n",
        "  tensorflow==1.15 \\\n",
        "  networkx  \\\n",
        "  imgaug \\\n",
        "  BlackBoxAuditing \\\n",
        "  adversarial-robustness-toolbox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Ply7t3AJ1O",
        "colab_type": "text"
      },
      "source": [
        "### Patch for pandas\n",
        "\n",
        "There is an issue with Pandas 1.x and the implementation of the `standard_dataset.py` where we have to convert a Pandas series to a numpy array. After you apply the patch you will need to restart the runtime. This can be done via the menu `Runtime > Restart runtime` or by using the shortcut `Ctrl + M .` You can then continue running the cells under."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R6WMg5zJ0Yt",
        "colab_type": "text"
      },
      "source": [
        "### Notes\n",
        "- The above pip command is created using AIF360's [requirements.txt](https://github.com/josephineHonore/AIF360/blob/master/requirements.txt). At the moment, the job to update these libraries is manual.\n",
        "- The original notebook uses Markdown to display formated text. Currently this is [unsupported](https://github.com/googlecolab/colabtools/issues/322) in Colab.\n",
        "- The tensorflow dependency is not needed for all other notebooks.\n",
        "- We have changed TensorFlow's logging level to `ERROR`, just after the import of the library, to limit the amount of logging shown to the user.\n",
        "- We have added code to fix the random seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJDtujOpATLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "cellView": "form",
        "outputId": "4de2b9b3-fc5f-4a04-ec90-cdc325a80cc5"
      },
      "source": [
        "#@title Run this cell to patch `standard_dataset.py`.\n",
        "%%writefile /usr/local/lib/python3.6/dist-packages/aif360/datasets/standard_dataset.py\n",
        "from logging import warning\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "\n",
        "\n",
        "class StandardDataset(BinaryLabelDataset):\n",
        "    \"\"\"Base class for every :obj:`BinaryLabelDataset` provided out of the box by\n",
        "    aif360.\n",
        "\n",
        "    It is not strictly necessary to inherit this class when adding custom\n",
        "    datasets but it may be useful.\n",
        "\n",
        "    This class is very loosely based on code from\n",
        "    https://github.com/algofairness/fairness-comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, label_name, favorable_classes,\n",
        "                 protected_attribute_names, privileged_classes,\n",
        "                 instance_weights_name='', scores_name='',\n",
        "                 categorical_features=[], features_to_keep=[],\n",
        "                 features_to_drop=[], na_values=[], custom_preprocessing=None,\n",
        "                 metadata=None):\n",
        "        \"\"\"\n",
        "        Subclasses of StandardDataset should perform the following before\n",
        "        calling `super().__init__`:\n",
        "\n",
        "            1. Load the dataframe from a raw file.\n",
        "\n",
        "        Then, this class will go through a standard preprocessing routine which:\n",
        "\n",
        "            2. (optional) Performs some dataset-specific preprocessing (e.g.\n",
        "               renaming columns/values, handling missing data).\n",
        "\n",
        "            3. Drops unrequested columns (see `features_to_keep` and\n",
        "               `features_to_drop` for details).\n",
        "\n",
        "            4. Drops rows with NA values.\n",
        "\n",
        "            5. Creates a one-hot encoding of the categorical variables.\n",
        "\n",
        "            6. Maps protected attributes to binary privileged/unprivileged\n",
        "               values (1/0).\n",
        "\n",
        "            7. Maps labels to binary favorable/unfavorable labels (1/0).\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): DataFrame on which to perform standard\n",
        "                processing.\n",
        "            label_name: Name of the label column in `df`.\n",
        "            favorable_classes (list or function): Label values which are\n",
        "                considered favorable or a boolean function which returns `True`\n",
        "                if favorable. All others are unfavorable. Label values are\n",
        "                mapped to 1 (favorable) and 0 (unfavorable) if they are not\n",
        "                already binary and numerical.\n",
        "            protected_attribute_names (list): List of names corresponding to\n",
        "                protected attribute columns in `df`.\n",
        "            privileged_classes (list(list or function)): Each element is\n",
        "                a list of values which are considered privileged or a boolean\n",
        "                function which return `True` if privileged for the corresponding\n",
        "                column in `protected_attribute_names`. All others are\n",
        "                unprivileged. Values are mapped to 1 (privileged) and 0\n",
        "                (unprivileged) if they are not already numerical.\n",
        "            instance_weights_name (optional): Name of the instance weights\n",
        "                column in `df`.\n",
        "            categorical_features (optional, list): List of column names in the\n",
        "                DataFrame which are to be expanded into one-hot vectors.\n",
        "            features_to_keep (optional, list): Column names to keep. All others\n",
        "                are dropped except those present in `protected_attribute_names`,\n",
        "                `categorical_features`, `label_name` or `instance_weights_name`.\n",
        "                Defaults to all columns if not provided.\n",
        "            features_to_drop (optional, list): Column names to drop. *Note: this\n",
        "                overrides* `features_to_keep`.\n",
        "            na_values (optional): Additional strings to recognize as NA. See\n",
        "                :func:`pandas.read_csv` for details.\n",
        "            custom_preprocessing (function): A function object which\n",
        "                acts on and returns a DataFrame (f: DataFrame -> DataFrame). If\n",
        "                `None`, no extra preprocessing is applied.\n",
        "            metadata (optional): Additional metadata to append.\n",
        "        \"\"\"\n",
        "        # 2. Perform dataset-specific preprocessing\n",
        "        if custom_preprocessing:\n",
        "            df = custom_preprocessing(df)\n",
        "\n",
        "        # 3. Drop unrequested columns\n",
        "        features_to_keep = features_to_keep or df.columns.tolist()\n",
        "        keep = (set(features_to_keep) | set(protected_attribute_names)\n",
        "              | set(categorical_features) | set([label_name]))\n",
        "        if instance_weights_name:\n",
        "            keep |= set([instance_weights_name])\n",
        "        df = df[sorted(keep - set(features_to_drop), key=df.columns.get_loc)]\n",
        "        categorical_features = sorted(set(categorical_features) - set(features_to_drop), key=df.columns.get_loc)\n",
        "\n",
        "        # 4. Remove any rows that have missing data.\n",
        "        dropped = df.dropna()\n",
        "        count = df.shape[0] - dropped.shape[0]\n",
        "        if count > 0:\n",
        "            warning(\"Missing Data: {} rows removed from {}.\".format(count,\n",
        "                    type(self).__name__))\n",
        "        df = dropped\n",
        "\n",
        "        # 5. Create a one-hot encoding of the categorical variables.\n",
        "        df = pd.get_dummies(df, columns=categorical_features, prefix_sep='=')\n",
        "\n",
        "        # 6. Map protected attributes to privileged/unprivileged\n",
        "        privileged_protected_attributes = []\n",
        "        unprivileged_protected_attributes = []\n",
        "        for attr, vals in zip(protected_attribute_names, privileged_classes):\n",
        "            privileged_values = [1.]\n",
        "            unprivileged_values = [0.]\n",
        "            if callable(vals):\n",
        "                df[attr] = df[attr].apply(vals)\n",
        "            elif np.issubdtype(df[attr].dtype, np.number):\n",
        "                # this attribute is numeric; no remapping needed\n",
        "                privileged_values = vals\n",
        "                unprivileged_values = list(set(df[attr]).difference(vals))\n",
        "            else:\n",
        "                # find all instances which match any of the attribute values\n",
        "                priv = np.logical_or.reduce(np.equal.outer(vals, df[attr].to_numpy()))\n",
        "                df.loc[priv, attr] = privileged_values[0]\n",
        "                df.loc[~priv, attr] = unprivileged_values[0]\n",
        "\n",
        "            privileged_protected_attributes.append(\n",
        "                np.array(privileged_values, dtype=np.float64))\n",
        "            unprivileged_protected_attributes.append(\n",
        "                np.array(unprivileged_values, dtype=np.float64))\n",
        "\n",
        "        # 7. Make labels binary\n",
        "        favorable_label = 1.\n",
        "        unfavorable_label = 0.\n",
        "        if callable(favorable_classes):\n",
        "            df[label_name] = df[label_name].apply(favorable_classes)\n",
        "        elif np.issubdtype(df[label_name], np.number) and len(set(df[label_name])) == 2:\n",
        "            # labels are already binary; don't change them\n",
        "            favorable_label = favorable_classes[0]\n",
        "            unfavorable_label = set(df[label_name]).difference(favorable_classes).pop()\n",
        "        else:\n",
        "            # find all instances which match any of the favorable classes\n",
        "            pos = np.logical_or.reduce(np.equal.outer(favorable_classes,\n",
        "                                                      df[label_name].to_numpy()))\n",
        "            df.loc[pos, label_name] = favorable_label\n",
        "            df.loc[~pos, label_name] = unfavorable_label\n",
        "\n",
        "        super(StandardDataset, self).__init__(df=df, label_names=[label_name],\n",
        "            protected_attribute_names=protected_attribute_names,\n",
        "            privileged_protected_attributes=privileged_protected_attributes,\n",
        "            unprivileged_protected_attributes=unprivileged_protected_attributes,\n",
        "            instance_weights_name=instance_weights_name,\n",
        "            scores_names=[scores_name] if scores_name else [],\n",
        "            favorable_label=favorable_label,\n",
        "            unfavorable_label=unfavorable_label, metadata=metadata)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/aif360/datasets/standard_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g_a1N5zW23l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printb(text):\n",
        "  \"\"\"Auxiliar function to print in bold.\n",
        "    Compensates for bug in Colab that doesn't show Markdown(diplay('text'))\n",
        "  \"\"\"\n",
        "  print('\\x1b[1;30m'+text+'\\x1b[0m')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQZm1kvfevkw",
        "colab_type": "text"
      },
      "source": [
        "# Start of Original Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI9JjWykdBgu",
        "colab_type": "text"
      },
      "source": [
        "#### This notebook demonstrates the use of adversarial debiasing algorithm to learn a fair classifier.\n",
        "Adversarial debiasing [1] is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit. We will see how to use this algorithm for learning models with and without fairness constraints and apply them on the Adult dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE04h_pXdBik",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "    References:\n",
        "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
        "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018.\n",
        "[article_link](https://arxiv.org/abs/1801.07593)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8X6I3szdBgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2b81604b-33c0-4696-d51f-95329837da09"
      },
      "source": [
        "%matplotlib inline\n",
        "# Load all necessary packages\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
        "\n",
        "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:2: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APNd5vJvXM9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)\n",
        "SEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcapCQdodBg8",
        "colab_type": "text"
      },
      "source": [
        "#### Load dataset and set options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8GyuuH-dBg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the dataset and split into train and test\n",
        "dataset_orig = load_preproc_data_adult()\n",
        "\n",
        "privileged_groups = [{'sex': 1}]\n",
        "unprivileged_groups = [{'sex': 0}]\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True, seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aePh7ARSdBhI",
        "colab_type": "code",
        "outputId": "ef881fa8-7bb4-481c-d37c-8b646db95e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# print out some labels, names, etc.\n",
        "#display(Markdown(\"#### Training Dataset shape\"))\n",
        "printb('#### Training Dataset shape')\n",
        "print(dataset_orig_train.features.shape)\n",
        "#display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
        "printb(\"#### Favorable and unfavorable labels\")\n",
        "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
        "#display(Markdown(\"#### Protected attribute names\"))\n",
        "printb(\"#### Protected attribute names\")\n",
        "print(dataset_orig_train.protected_attribute_names)\n",
        "#display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
        "printb(\"#### Privileged and unprivileged protected attribute values\")\n",
        "print(dataset_orig_train.privileged_protected_attributes, \n",
        "      dataset_orig_train.unprivileged_protected_attributes)\n",
        "#display(Markdown(\"#### Dataset feature names\"))\n",
        "printb(\"#### Dataset feature names\")\n",
        "print(dataset_orig_train.feature_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m#### Training Dataset shape\u001b[0m\n",
            "(34189, 18)\n",
            "\u001b[1;30m#### Favorable and unfavorable labels\u001b[0m\n",
            "1.0 0.0\n",
            "\u001b[1;30m#### Protected attribute names\u001b[0m\n",
            "['sex', 'race']\n",
            "\u001b[1;30m#### Privileged and unprivileged protected attribute values\u001b[0m\n",
            "[array([1.]), array([1.])] [array([0.]), array([0.])]\n",
            "\u001b[1;30m#### Dataset feature names\u001b[0m\n",
            "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_FiXzGdBhQ",
        "colab_type": "text"
      },
      "source": [
        "#### Metric for original training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEjeW5bIdBhS",
        "colab_type": "code",
        "outputId": "eecedfec-0911-4af2-c177-26f9d602acd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Metric for the original dataset\n",
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "#display(Markdown(\"#### Original training dataset\"))\n",
        "printb(\"#### Original training dataset\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
        "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m#### Original training dataset\u001b[0m\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.195979\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.191102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RGDBY2RdBhZ",
        "colab_type": "code",
        "outputId": "0bd3291c-9ef8-45b9-d25a-83541dfe155b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "min_max_scaler = MaxAbsScaler()\n",
        "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
        "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
        "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                             unprivileged_groups=unprivileged_groups,\n",
        "                             privileged_groups=privileged_groups)\n",
        "#display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
        "printb(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
        "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
        "                             unprivileged_groups=unprivileged_groups,\n",
        "                             privileged_groups=privileged_groups)\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m#### Scaled dataset - Verify that the scaling does not affect the group label statistics\u001b[0m\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.195979\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.191102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Yli0jndBhh",
        "colab_type": "text"
      },
      "source": [
        "### Learn plan classifier without debiasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlAILO8edBhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load post-processing algorithm that equalizes the odds\n",
        "# Learn parameters with debias set to False\n",
        "sess = tf.Session()\n",
        "tf.set_random_seed(SEED)\n",
        "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='plain_classifier',\n",
        "                          debias=False,\n",
        "                          sess=sess,\n",
        "                          seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kDacKLdtdBhr",
        "colab_type": "code",
        "outputId": "2f870020-ac1a-4666-9df1-630c98927e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Default title text\n",
        "plain_model.fit(dataset_orig_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.737970\n",
            "epoch 0; iter: 200; batch classifier loss: 0.438434\n",
            "epoch 1; iter: 0; batch classifier loss: 0.376381\n",
            "epoch 1; iter: 200; batch classifier loss: 0.418228\n",
            "epoch 2; iter: 0; batch classifier loss: 0.385003\n",
            "epoch 2; iter: 200; batch classifier loss: 0.609906\n",
            "epoch 3; iter: 0; batch classifier loss: 0.352963\n",
            "epoch 3; iter: 200; batch classifier loss: 0.464395\n",
            "epoch 4; iter: 0; batch classifier loss: 0.438074\n",
            "epoch 4; iter: 200; batch classifier loss: 0.466126\n",
            "epoch 5; iter: 0; batch classifier loss: 0.423859\n",
            "epoch 5; iter: 200; batch classifier loss: 0.453185\n",
            "epoch 6; iter: 0; batch classifier loss: 0.415533\n",
            "epoch 6; iter: 200; batch classifier loss: 0.472421\n",
            "epoch 7; iter: 0; batch classifier loss: 0.499586\n",
            "epoch 7; iter: 200; batch classifier loss: 0.404337\n",
            "epoch 8; iter: 0; batch classifier loss: 0.476573\n",
            "epoch 8; iter: 200; batch classifier loss: 0.430814\n",
            "epoch 9; iter: 0; batch classifier loss: 0.383572\n",
            "epoch 9; iter: 200; batch classifier loss: 0.442796\n",
            "epoch 10; iter: 0; batch classifier loss: 0.418980\n",
            "epoch 10; iter: 200; batch classifier loss: 0.365440\n",
            "epoch 11; iter: 0; batch classifier loss: 0.374632\n",
            "epoch 11; iter: 200; batch classifier loss: 0.340077\n",
            "epoch 12; iter: 0; batch classifier loss: 0.471585\n",
            "epoch 12; iter: 200; batch classifier loss: 0.461701\n",
            "epoch 13; iter: 0; batch classifier loss: 0.411233\n",
            "epoch 13; iter: 200; batch classifier loss: 0.382886\n",
            "epoch 14; iter: 0; batch classifier loss: 0.501461\n",
            "epoch 14; iter: 200; batch classifier loss: 0.399482\n",
            "epoch 15; iter: 0; batch classifier loss: 0.492417\n",
            "epoch 15; iter: 200; batch classifier loss: 0.420455\n",
            "epoch 16; iter: 0; batch classifier loss: 0.337521\n",
            "epoch 16; iter: 200; batch classifier loss: 0.326661\n",
            "epoch 17; iter: 0; batch classifier loss: 0.382796\n",
            "epoch 17; iter: 200; batch classifier loss: 0.394141\n",
            "epoch 18; iter: 0; batch classifier loss: 0.466832\n",
            "epoch 18; iter: 200; batch classifier loss: 0.450622\n",
            "epoch 19; iter: 0; batch classifier loss: 0.470818\n",
            "epoch 19; iter: 200; batch classifier loss: 0.489971\n",
            "epoch 20; iter: 0; batch classifier loss: 0.435294\n",
            "epoch 20; iter: 200; batch classifier loss: 0.400488\n",
            "epoch 21; iter: 0; batch classifier loss: 0.468574\n",
            "epoch 21; iter: 200; batch classifier loss: 0.387850\n",
            "epoch 22; iter: 0; batch classifier loss: 0.440267\n",
            "epoch 22; iter: 200; batch classifier loss: 0.396217\n",
            "epoch 23; iter: 0; batch classifier loss: 0.516843\n",
            "epoch 23; iter: 200; batch classifier loss: 0.376769\n",
            "epoch 24; iter: 0; batch classifier loss: 0.455482\n",
            "epoch 24; iter: 200; batch classifier loss: 0.428885\n",
            "epoch 25; iter: 0; batch classifier loss: 0.350363\n",
            "epoch 25; iter: 200; batch classifier loss: 0.359157\n",
            "epoch 26; iter: 0; batch classifier loss: 0.444447\n",
            "epoch 26; iter: 200; batch classifier loss: 0.509682\n",
            "epoch 27; iter: 0; batch classifier loss: 0.434386\n",
            "epoch 27; iter: 200; batch classifier loss: 0.440993\n",
            "epoch 28; iter: 0; batch classifier loss: 0.473746\n",
            "epoch 28; iter: 200; batch classifier loss: 0.438024\n",
            "epoch 29; iter: 0; batch classifier loss: 0.406484\n",
            "epoch 29; iter: 200; batch classifier loss: 0.454816\n",
            "epoch 30; iter: 0; batch classifier loss: 0.436453\n",
            "epoch 30; iter: 200; batch classifier loss: 0.305891\n",
            "epoch 31; iter: 0; batch classifier loss: 0.410468\n",
            "epoch 31; iter: 200; batch classifier loss: 0.400559\n",
            "epoch 32; iter: 0; batch classifier loss: 0.445880\n",
            "epoch 32; iter: 200; batch classifier loss: 0.393083\n",
            "epoch 33; iter: 0; batch classifier loss: 0.535262\n",
            "epoch 33; iter: 200; batch classifier loss: 0.367664\n",
            "epoch 34; iter: 0; batch classifier loss: 0.380545\n",
            "epoch 34; iter: 200; batch classifier loss: 0.352619\n",
            "epoch 35; iter: 0; batch classifier loss: 0.414669\n",
            "epoch 35; iter: 200; batch classifier loss: 0.397442\n",
            "epoch 36; iter: 0; batch classifier loss: 0.426102\n",
            "epoch 36; iter: 200; batch classifier loss: 0.402349\n",
            "epoch 37; iter: 0; batch classifier loss: 0.388710\n",
            "epoch 37; iter: 200; batch classifier loss: 0.470019\n",
            "epoch 38; iter: 0; batch classifier loss: 0.363627\n",
            "epoch 38; iter: 200; batch classifier loss: 0.439162\n",
            "epoch 39; iter: 0; batch classifier loss: 0.425166\n",
            "epoch 39; iter: 200; batch classifier loss: 0.440525\n",
            "epoch 40; iter: 0; batch classifier loss: 0.420522\n",
            "epoch 40; iter: 200; batch classifier loss: 0.418079\n",
            "epoch 41; iter: 0; batch classifier loss: 0.470677\n",
            "epoch 41; iter: 200; batch classifier loss: 0.441284\n",
            "epoch 42; iter: 0; batch classifier loss: 0.399362\n",
            "epoch 42; iter: 200; batch classifier loss: 0.427640\n",
            "epoch 43; iter: 0; batch classifier loss: 0.530691\n",
            "epoch 43; iter: 200; batch classifier loss: 0.444675\n",
            "epoch 44; iter: 0; batch classifier loss: 0.405654\n",
            "epoch 44; iter: 200; batch classifier loss: 0.484172\n",
            "epoch 45; iter: 0; batch classifier loss: 0.512941\n",
            "epoch 45; iter: 200; batch classifier loss: 0.419159\n",
            "epoch 46; iter: 0; batch classifier loss: 0.513867\n",
            "epoch 46; iter: 200; batch classifier loss: 0.482165\n",
            "epoch 47; iter: 0; batch classifier loss: 0.436052\n",
            "epoch 47; iter: 200; batch classifier loss: 0.331856\n",
            "epoch 48; iter: 0; batch classifier loss: 0.426657\n",
            "epoch 48; iter: 200; batch classifier loss: 0.449841\n",
            "epoch 49; iter: 0; batch classifier loss: 0.455308\n",
            "epoch 49; iter: 200; batch classifier loss: 0.399962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f8f5e5521d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1x0uGG4dBhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the plain model to test data\n",
        "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
        "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLKA0ALndBh3",
        "colab_type": "code",
        "outputId": "ff4c0492-5955-4991-8590-2cf6c09335c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Metrics for the dataset from plain model (without debiasing)\n",
        "#display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
        "printb(\"#### Plain model - without debiasing - dataset metrics\")\n",
        "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
        "\n",
        "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
        "\n",
        "#display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
        "printb(\"#### Plain model - without debiasing - classification metrics\")\n",
        "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
        "                                                 dataset_nodebiasing_test,\n",
        "                                                 unprivileged_groups=unprivileged_groups,\n",
        "                                                 privileged_groups=privileged_groups)\n",
        "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
        "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
        "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
        "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
        "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
        "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
        "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
        "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
        "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m#### Plain model - without debiasing - dataset metrics\u001b[0m\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.230944\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.233361\n",
            "\u001b[1;30m#### Plain model - without debiasing - classification metrics\u001b[0m\n",
            "Test set: Classification accuracy = 0.801679\n",
            "Test set: Balanced classification accuracy = 0.666963\n",
            "Test set: Disparate impact = 0.000000\n",
            "Test set: Equal opportunity difference = -0.484879\n",
            "Test set: Average odds difference = -0.305113\n",
            "Test set: Theil_index = 0.172990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXaAnczRdBh-",
        "colab_type": "text"
      },
      "source": [
        "### Apply in-processing algorithm based on adversarial learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBI5i5BvdBiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "tf.set_random_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZLhz4WVdBiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learn parameters with debias set to True\n",
        "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='debiased_classifier',\n",
        "                          debias=True,\n",
        "                          sess=sess,\n",
        "                          seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qhydrm-PdBiL",
        "colab_type": "code",
        "outputId": "c16bad00-c57c-4511-d764-95775278df05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "debiased_model.fit(dataset_orig_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0; iter: 0; batch classifier loss: 0.737970; batch adversarial loss: 0.666912\n",
            "epoch 0; iter: 200; batch classifier loss: 0.439279; batch adversarial loss: 0.650546\n",
            "epoch 1; iter: 0; batch classifier loss: 0.395545; batch adversarial loss: 0.653748\n",
            "epoch 1; iter: 200; batch classifier loss: 0.398371; batch adversarial loss: 0.632805\n",
            "epoch 2; iter: 0; batch classifier loss: 0.393818; batch adversarial loss: 0.595920\n",
            "epoch 2; iter: 200; batch classifier loss: 0.648361; batch adversarial loss: 0.623091\n",
            "epoch 3; iter: 0; batch classifier loss: 0.388254; batch adversarial loss: 0.640224\n",
            "epoch 3; iter: 200; batch classifier loss: 0.557428; batch adversarial loss: 0.648691\n",
            "epoch 4; iter: 0; batch classifier loss: 0.497622; batch adversarial loss: 0.605873\n",
            "epoch 4; iter: 200; batch classifier loss: 0.535735; batch adversarial loss: 0.627681\n",
            "epoch 5; iter: 0; batch classifier loss: 0.520497; batch adversarial loss: 0.671775\n",
            "epoch 5; iter: 200; batch classifier loss: 0.516437; batch adversarial loss: 0.577044\n",
            "epoch 6; iter: 0; batch classifier loss: 0.419325; batch adversarial loss: 0.689396\n",
            "epoch 6; iter: 200; batch classifier loss: 0.505923; batch adversarial loss: 0.614437\n",
            "epoch 7; iter: 0; batch classifier loss: 0.540205; batch adversarial loss: 0.618127\n",
            "epoch 7; iter: 200; batch classifier loss: 0.421811; batch adversarial loss: 0.620169\n",
            "epoch 8; iter: 0; batch classifier loss: 0.507069; batch adversarial loss: 0.650331\n",
            "epoch 8; iter: 200; batch classifier loss: 0.467451; batch adversarial loss: 0.588098\n",
            "epoch 9; iter: 0; batch classifier loss: 0.408772; batch adversarial loss: 0.622584\n",
            "epoch 9; iter: 200; batch classifier loss: 0.454897; batch adversarial loss: 0.611075\n",
            "epoch 10; iter: 0; batch classifier loss: 0.428160; batch adversarial loss: 0.597811\n",
            "epoch 10; iter: 200; batch classifier loss: 0.378412; batch adversarial loss: 0.582746\n",
            "epoch 11; iter: 0; batch classifier loss: 0.399729; batch adversarial loss: 0.616520\n",
            "epoch 11; iter: 200; batch classifier loss: 0.332805; batch adversarial loss: 0.641569\n",
            "epoch 12; iter: 0; batch classifier loss: 0.496957; batch adversarial loss: 0.629409\n",
            "epoch 12; iter: 200; batch classifier loss: 0.490906; batch adversarial loss: 0.586128\n",
            "epoch 13; iter: 0; batch classifier loss: 0.458639; batch adversarial loss: 0.619904\n",
            "epoch 13; iter: 200; batch classifier loss: 0.383926; batch adversarial loss: 0.576745\n",
            "epoch 14; iter: 0; batch classifier loss: 0.521511; batch adversarial loss: 0.561820\n",
            "epoch 14; iter: 200; batch classifier loss: 0.411558; batch adversarial loss: 0.615814\n",
            "epoch 15; iter: 0; batch classifier loss: 0.485787; batch adversarial loss: 0.624828\n",
            "epoch 15; iter: 200; batch classifier loss: 0.421979; batch adversarial loss: 0.615305\n",
            "epoch 16; iter: 0; batch classifier loss: 0.338333; batch adversarial loss: 0.687905\n",
            "epoch 16; iter: 200; batch classifier loss: 0.350561; batch adversarial loss: 0.576916\n",
            "epoch 17; iter: 0; batch classifier loss: 0.375662; batch adversarial loss: 0.630556\n",
            "epoch 17; iter: 200; batch classifier loss: 0.416972; batch adversarial loss: 0.562591\n",
            "epoch 18; iter: 0; batch classifier loss: 0.483189; batch adversarial loss: 0.583048\n",
            "epoch 18; iter: 200; batch classifier loss: 0.457346; batch adversarial loss: 0.573733\n",
            "epoch 19; iter: 0; batch classifier loss: 0.468672; batch adversarial loss: 0.618612\n",
            "epoch 19; iter: 200; batch classifier loss: 0.480956; batch adversarial loss: 0.548824\n",
            "epoch 20; iter: 0; batch classifier loss: 0.430802; batch adversarial loss: 0.678742\n",
            "epoch 20; iter: 200; batch classifier loss: 0.423486; batch adversarial loss: 0.566924\n",
            "epoch 21; iter: 0; batch classifier loss: 0.467919; batch adversarial loss: 0.683992\n",
            "epoch 21; iter: 200; batch classifier loss: 0.434601; batch adversarial loss: 0.587634\n",
            "epoch 22; iter: 0; batch classifier loss: 0.439948; batch adversarial loss: 0.593956\n",
            "epoch 22; iter: 200; batch classifier loss: 0.422943; batch adversarial loss: 0.588045\n",
            "epoch 23; iter: 0; batch classifier loss: 0.499347; batch adversarial loss: 0.574031\n",
            "epoch 23; iter: 200; batch classifier loss: 0.394045; batch adversarial loss: 0.609125\n",
            "epoch 24; iter: 0; batch classifier loss: 0.479658; batch adversarial loss: 0.628755\n",
            "epoch 24; iter: 200; batch classifier loss: 0.456394; batch adversarial loss: 0.577668\n",
            "epoch 25; iter: 0; batch classifier loss: 0.354100; batch adversarial loss: 0.642074\n",
            "epoch 25; iter: 200; batch classifier loss: 0.347540; batch adversarial loss: 0.646444\n",
            "epoch 26; iter: 0; batch classifier loss: 0.480685; batch adversarial loss: 0.575224\n",
            "epoch 26; iter: 200; batch classifier loss: 0.527379; batch adversarial loss: 0.638469\n",
            "epoch 27; iter: 0; batch classifier loss: 0.432058; batch adversarial loss: 0.618656\n",
            "epoch 27; iter: 200; batch classifier loss: 0.445201; batch adversarial loss: 0.654056\n",
            "epoch 28; iter: 0; batch classifier loss: 0.503717; batch adversarial loss: 0.573264\n",
            "epoch 28; iter: 200; batch classifier loss: 0.421909; batch adversarial loss: 0.578267\n",
            "epoch 29; iter: 0; batch classifier loss: 0.405094; batch adversarial loss: 0.597576\n",
            "epoch 29; iter: 200; batch classifier loss: 0.472813; batch adversarial loss: 0.542019\n",
            "epoch 30; iter: 0; batch classifier loss: 0.458798; batch adversarial loss: 0.524102\n",
            "epoch 30; iter: 200; batch classifier loss: 0.323843; batch adversarial loss: 0.643671\n",
            "epoch 31; iter: 0; batch classifier loss: 0.445980; batch adversarial loss: 0.638868\n",
            "epoch 31; iter: 200; batch classifier loss: 0.417058; batch adversarial loss: 0.625491\n",
            "epoch 32; iter: 0; batch classifier loss: 0.452058; batch adversarial loss: 0.575517\n",
            "epoch 32; iter: 200; batch classifier loss: 0.389214; batch adversarial loss: 0.641523\n",
            "epoch 33; iter: 0; batch classifier loss: 0.526390; batch adversarial loss: 0.602945\n",
            "epoch 33; iter: 200; batch classifier loss: 0.397207; batch adversarial loss: 0.618547\n",
            "epoch 34; iter: 0; batch classifier loss: 0.373913; batch adversarial loss: 0.653317\n",
            "epoch 34; iter: 200; batch classifier loss: 0.367177; batch adversarial loss: 0.638157\n",
            "epoch 35; iter: 0; batch classifier loss: 0.425128; batch adversarial loss: 0.673568\n",
            "epoch 35; iter: 200; batch classifier loss: 0.419169; batch adversarial loss: 0.643450\n",
            "epoch 36; iter: 0; batch classifier loss: 0.441010; batch adversarial loss: 0.558340\n",
            "epoch 36; iter: 200; batch classifier loss: 0.429690; batch adversarial loss: 0.611624\n",
            "epoch 37; iter: 0; batch classifier loss: 0.407387; batch adversarial loss: 0.681736\n",
            "epoch 37; iter: 200; batch classifier loss: 0.486381; batch adversarial loss: 0.594936\n",
            "epoch 38; iter: 0; batch classifier loss: 0.381194; batch adversarial loss: 0.643299\n",
            "epoch 38; iter: 200; batch classifier loss: 0.443554; batch adversarial loss: 0.590300\n",
            "epoch 39; iter: 0; batch classifier loss: 0.467870; batch adversarial loss: 0.551546\n",
            "epoch 39; iter: 200; batch classifier loss: 0.463101; batch adversarial loss: 0.642417\n",
            "epoch 40; iter: 0; batch classifier loss: 0.429701; batch adversarial loss: 0.680142\n",
            "epoch 40; iter: 200; batch classifier loss: 0.456982; batch adversarial loss: 0.572638\n",
            "epoch 41; iter: 0; batch classifier loss: 0.494551; batch adversarial loss: 0.547362\n",
            "epoch 41; iter: 200; batch classifier loss: 0.439439; batch adversarial loss: 0.628853\n",
            "epoch 42; iter: 0; batch classifier loss: 0.398277; batch adversarial loss: 0.604598\n",
            "epoch 42; iter: 200; batch classifier loss: 0.452474; batch adversarial loss: 0.557863\n",
            "epoch 43; iter: 0; batch classifier loss: 0.553473; batch adversarial loss: 0.574909\n",
            "epoch 43; iter: 200; batch classifier loss: 0.464678; batch adversarial loss: 0.614460\n",
            "epoch 44; iter: 0; batch classifier loss: 0.423944; batch adversarial loss: 0.631796\n",
            "epoch 44; iter: 200; batch classifier loss: 0.480080; batch adversarial loss: 0.648881\n",
            "epoch 45; iter: 0; batch classifier loss: 0.520836; batch adversarial loss: 0.545991\n",
            "epoch 45; iter: 200; batch classifier loss: 0.427333; batch adversarial loss: 0.607846\n",
            "epoch 46; iter: 0; batch classifier loss: 0.513178; batch adversarial loss: 0.652858\n",
            "epoch 46; iter: 200; batch classifier loss: 0.496726; batch adversarial loss: 0.576679\n",
            "epoch 47; iter: 0; batch classifier loss: 0.445464; batch adversarial loss: 0.606022\n",
            "epoch 47; iter: 200; batch classifier loss: 0.344738; batch adversarial loss: 0.649998\n",
            "epoch 48; iter: 0; batch classifier loss: 0.452124; batch adversarial loss: 0.567727\n",
            "epoch 48; iter: 200; batch classifier loss: 0.438918; batch adversarial loss: 0.646284\n",
            "epoch 49; iter: 0; batch classifier loss: 0.464006; batch adversarial loss: 0.572272\n",
            "epoch 49; iter: 200; batch classifier loss: 0.414751; batch adversarial loss: 0.564759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f8f52299cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSL33flZdBiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the plain model to test data\n",
        "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
        "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3izs52oMdBia",
        "colab_type": "code",
        "outputId": "e4bbfa06-9ef6-4398-e0dc-d623bf2a8c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Metrics for the dataset from plain model (without debiasing)\n",
        "#display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
        "printb(\"#### Plain model - without debiasing - dataset metrics\")\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
        "\n",
        "# Metrics for the dataset from model with debiasing\n",
        "#display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
        "printb(\"#### Model - with debiasing - dataset metrics\")\n",
        "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
        "\n",
        "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "\n",
        "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
        "\n",
        "\n",
        "\n",
        "#display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
        "printb(\"#### Plain model - without debiasing - classification metrics\")\n",
        "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
        "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
        "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
        "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
        "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
        "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
        "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
        "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
        "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
        "\n",
        "\n",
        "\n",
        "#display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
        "printb(\"#### Model - with debiasing - classification metrics\")\n",
        "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
        "                                                 dataset_debiasing_test,\n",
        "                                                 unprivileged_groups=unprivileged_groups,\n",
        "                                                 privileged_groups=privileged_groups)\n",
        "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
        "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
        "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
        "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
        "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
        "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
        "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
        "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
        "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m#### Plain model - without debiasing - dataset metrics\u001b[0m\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.230944\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.233361\n",
            "\u001b[1;30m#### Model - with debiasing - dataset metrics\u001b[0m\n",
            "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.086433\n",
            "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.084841\n",
            "\u001b[1;30m#### Plain model - without debiasing - classification metrics\u001b[0m\n",
            "Test set: Classification accuracy = 0.801679\n",
            "Test set: Balanced classification accuracy = 0.666963\n",
            "Test set: Disparate impact = 0.000000\n",
            "Test set: Equal opportunity difference = -0.484879\n",
            "Test set: Average odds difference = -0.305113\n",
            "Test set: Theil_index = 0.172990\n",
            "\u001b[1;30m#### Model - with debiasing - classification metrics\u001b[0m\n",
            "Test set: Classification accuracy = 0.790145\n",
            "Test set: Balanced classification accuracy = 0.666546\n",
            "Test set: Disparate impact = 0.587130\n",
            "Test set: Equal opportunity difference = -0.053623\n",
            "Test set: Average odds difference = -0.035234\n",
            "Test set: Theil_index = 0.172250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHgmAjjLEZuB",
        "colab_type": "text"
      },
      "source": [
        "# Exploring the results\n",
        "\n",
        "Let's take a deeper look at the previous results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54j99wHteilZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run cell to define `print_table` function to show results in tabular format\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def print_table(headers,data,caption=\"\"):\n",
        "  \"\"\"\n",
        "  Prints a table given headers and data\n",
        "\n",
        "  Inputs:\n",
        "    - headers: a list of N headers\n",
        "    - data: a list of N-element lists containing the data to display\n",
        "    - caption: a string describing the data\n",
        "\n",
        "  Outputs:\n",
        "    - A HTML display of the table\n",
        "\n",
        "  Example:\n",
        "    caption = \"A caption\"\n",
        "    headers = [\"row\",\"title 1\", \"title 2\"]\n",
        "    data = [[\"first row\", 1, 2], [\"second row\", 2, 3]]\n",
        "\n",
        "    print_table(headers,data,caption)\n",
        "    \n",
        "\n",
        "         A Caption\n",
        "    -----------------------------------\n",
        "    | row         | title 1 | title 2 |\n",
        "    -----------------------------------\n",
        "    | first row   | 1       | 2       |\n",
        "    -----------------------------------\n",
        "    | second row  | 2       | 3       |\n",
        "    -----------------------------------\n",
        "  \"\"\"\n",
        "  display(HTML(\n",
        "    '<table border=\"1\"><caption>{0}</caption><tr>{1}</tr><tr>{2}</tr></table>'.format(\n",
        "        caption,\n",
        "        '<th>{}</th>'.format('</th><th>'.join(line for line in headers)),\n",
        "        '</tr><tr>'.join(\n",
        "            '<td>{}</td>'.format(\n",
        "                '</td><td>'.join(\n",
        "                    str(_) for _ in row)) for row in data))\n",
        "  ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EAi-ReJ0s4D",
        "colab_type": "code",
        "outputId": "d983d267-eb7d-458f-9390-94ad09a7ab79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "t_00 = \"{:.4f}\".format(metric_dataset_nodebiasing_train.mean_difference())\n",
        "t_01 = \"{:.4f}\".format(metric_dataset_debiasing_train.mean_difference())\n",
        "t_10 = \"{:.4f}\".format(metric_dataset_nodebiasing_test.mean_difference())\n",
        "t_11 = \"{:.4f}\".format(metric_dataset_debiasing_test.mean_difference())\n",
        "\n",
        "table = [[\"Train set\",t_00,t_01],\n",
        "         [\"Test set\",t_10,t_11]]\n",
        "headers = ['Statistical parity difference','Without debiasing','With debiasing']\n",
        "caption = \"Difference in mean outcomes between unprivileged and privileged groups\"\n",
        "\n",
        "print_table(headers,table,caption)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\"><caption>Difference in mean outcomes between unprivileged and privileged groups</caption><tr><th>Statistical parity difference</th><th>Without debiasing</th><th>With debiasing</th></tr><tr><td>Train set</td><td>-0.2309</td><td>-0.0864</td></tr><tr><td>Test set</td><td>-0.2334</td><td>-0.0848</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXnjQvpJFPPD",
        "colab_type": "text"
      },
      "source": [
        "We observe a big reduction in the statistical parity difference by training with Adversarial learning debias mitigation. \n",
        "\n",
        "Let's look at the result of this technique by evaluating other fairness metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kphsnDT6v9F7",
        "colab_type": "code",
        "outputId": "04918b47-b40a-4f72-b9ed-86859fd7af76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "acc_train_non_debias = \"{:.4f}\".format(classified_metric_nodebiasing_test.accuracy())\n",
        "acc_train_debias = \"{:.4f}\".format(classified_metric_debiasing_test.accuracy())\n",
        "acc_difference = \"{:.4f}\".format(classified_metric_nodebiasing_test.accuracy() - classified_metric_debiasing_test.accuracy())\n",
        "\n",
        "bal_acc_non_debias = \"{:.4f}\".format(bal_acc_nodebiasing_test)\n",
        "bal_acc_debias = \"{:.4f}\".format(bal_acc_debiasing_test)\n",
        "bal_acc_difference = \"{:.4f}\".format(bal_acc_nodebiasing_test - bal_acc_debiasing_test)\n",
        "\n",
        "di_non_debias = \"{:.4f}\".format(classified_metric_nodebiasing_test.disparate_impact())\n",
        "di_debias = \"{:.4f}\".format(classified_metric_debiasing_test.disparate_impact())\n",
        "# Remember that disparate impact is best when it is closest to 1. Thus we see how far is the result from 1 now.\n",
        "di_difference = \"{:.4f}\".format(1 + classified_metric_nodebiasing_test.disparate_impact() - classified_metric_debiasing_test.disparate_impact())\n",
        "\n",
        "eq_op_non_debias = \"{:.4f}\".format(classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
        "eq_op_debias = \"{:.4f}\".format(classified_metric_debiasing_test.equal_opportunity_difference())\n",
        "eq_op_difference = \"{:.4f}\".format(classified_metric_nodebiasing_test.equal_opportunity_difference() - classified_metric_debiasing_test.equal_opportunity_difference())\n",
        "\n",
        "avg_odds_non_debias = \"{:.4f}\".format(classified_metric_nodebiasing_test.average_odds_difference())\n",
        "avg_odds_debias = \"{:.4f}\".format(classified_metric_debiasing_test.average_odds_difference())\n",
        "avg_odds_difference = \"{:.4f}\".format(classified_metric_nodebiasing_test.average_odds_difference() - classified_metric_debiasing_test.average_odds_difference())\n",
        "\n",
        "theil_non_debias = \"{:.4f}\".format(classified_metric_nodebiasing_test.theil_index()) \n",
        "theil_debias = \"{:.4f}\".format(classified_metric_debiasing_test.theil_index())\n",
        "theil_difference = \"{:.4f}\".format(classified_metric_nodebiasing_test.theil_index() - classified_metric_debiasing_test.theil_index())\n",
        "\n",
        "metrics_final = [[\"Accuracy\", acc_train_non_debias, acc_train_debias, acc_difference],\n",
        "                [\"Balanced classification accuracy\", bal_acc_non_debias, bal_acc_debias, bal_acc_difference],\n",
        "                [\"Disparate impact\", di_non_debias, di_debias, di_difference],\n",
        "                [\"Equal opportunity difference\", eq_op_non_debias, eq_op_debias, eq_op_difference],\n",
        "                [\"Average odds difference\",avg_odds_non_debias , avg_odds_debias, avg_odds_difference],\n",
        "                [\"Theil_index\", theil_non_debias, theil_debias, theil_difference]]\n",
        "headers_final = [\"Classification metric\", \"Without debiasing\",\"With debiasing\", \"Distance\"]\n",
        "caption_final = \"Difference in model performance by using Adversarial Learning mitigation\"\n",
        "\n",
        "print_table(headers_final, metrics_final, caption_final)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\"><caption>Difference in model performance by using Adversarial Learning mitigation</caption><tr><th>Classification metric</th><th>Without debiasing</th><th>With debiasing</th><th>Distance</th></tr><tr><td>Accuracy</td><td>0.8017</td><td>0.7901</td><td>0.0115</td></tr><tr><td>Balanced classification accuracy</td><td>0.6670</td><td>0.6665</td><td>0.0004</td></tr><tr><td>Disparate impact</td><td>0.0000</td><td>0.5871</td><td>0.4129</td></tr><tr><td>Equal opportunity difference</td><td>-0.4849</td><td>-0.0536</td><td>-0.4313</td></tr><tr><td>Average odds difference</td><td>-0.3051</td><td>-0.0352</td><td>-0.2699</td></tr><tr><td>Theil_index</td><td>0.1730</td><td>0.1723</td><td>0.0007</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXSfLelFS2oz",
        "colab_type": "text"
      },
      "source": [
        "It is hard to remember the definition and the ideal expected value for each metric. We can use [explainers](https://aif360.readthedocs.io/en/latest/modules/explainers.html#) to explain each metric. There are two kind of flavours: TEXT and JSON. The JSON explainers provide structured explanations that can be used to present information to the users. Here are some examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpjQceTVEUd4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "def format_json(json_str):\n",
        "  return json.dumps(json.loads(json_str, object_pairs_hook=OrderedDict), indent=2)\n",
        "\n",
        "def get_ideal_value(metric_object):\n",
        "  return json.loads(metric_object)[\"ideal\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqwaTnO6QwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from aif360.explainers import MetricJSONExplainer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7kJThIUtdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define explainers for the metrics with and without debiasing\n",
        "ex_nondebias_test = MetricJSONExplainer(classified_metric_nodebiasing_test)\n",
        "ex_debias_test = MetricJSONExplainer(classified_metric_debiasing_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0o1lmC1VCbM",
        "colab_type": "text"
      },
      "source": [
        "Now let's print the explainers for the metrics we used above. Make sure you read the whole text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqUZfgF1EXbY",
        "colab_type": "code",
        "outputId": "88627ed4-8a2e-466a-c715-3228b1aeadfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "printb(\"\\n\\nAccuracy\\n\\n\")\n",
        "printb(\"Nondebiasing\")\n",
        "print(format_json(ex_nondebias_test.accuracy()))\n",
        "printb(\"Debiasing\")\n",
        "print(format_json(ex_debias_test.accuracy()))\n",
        "printb(\"\\n\\n################\\n\\n\")\n",
        "\n",
        "printb(\"\\n\\nDisparate_Impact\\n\\n\")\n",
        "printb(\"Nondebiasing\")\n",
        "print(format_json(ex_nondebias_test.disparate_impact()))\n",
        "printb(\"Debiasing\")\n",
        "print(format_json(ex_debias_test.disparate_impact()))\n",
        "printb(\"\\n\\n################\\n\\n\")\n",
        "\n",
        "printb(\"\\n\\nEqual_opportunity\\n\\n\")\n",
        "printb(\"Nondebiasing\")\n",
        "print(format_json(ex_nondebias_test.equal_opportunity_difference()))\n",
        "printb(\"Debiasing\")\n",
        "print(format_json(ex_debias_test.equal_opportunity_difference()))\n",
        "printb(\"\\n\\n################\\n\\n\")\n",
        "\n",
        "printb(\"\\n\\nAverage_odds\\n\\n\")\n",
        "printb(\"Nondebiasing\")\n",
        "print(format_json(ex_nondebias_test.average_odds_difference()))\n",
        "printb(\"Debiasing\")\n",
        "print(format_json(ex_debias_test.average_odds_difference()))\n",
        "printb(\"\\n\\n################\\n\\n\")\n",
        "\n",
        "printb(\"\\n\\nTheil_index\\n\\n\")\n",
        "printb(\"Nondebiasing\")\n",
        "print(format_json(ex_nondebias_test.theil_index()))\n",
        "printb(\"Debiasing\")\n",
        "print(format_json(ex_debias_test.theil_index()))\n",
        "printb(\"\\n\\n################\\n\\n\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30m\n",
            "\n",
            "Accuracy\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30mNondebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Accuracy\",\n",
            "  \"message\": \"Classification accuracy (ACC): 0.8016788370982052\",\n",
            "  \"numTruePositives\": 1427.0,\n",
            "  \"numTrueNegatives\": 10320.0,\n",
            "  \"numPositives\": 3474.0,\n",
            "  \"numNegatives\": 11179.0,\n",
            "  \"description\": \"Computed as (true positive count + true negative count)/(positive_count + negative_count).\",\n",
            "  \"ideal\": \"The ideal value of this metric is 1.0\"\n",
            "}\n",
            "\u001b[1;30mDebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Accuracy\",\n",
            "  \"message\": \"Classification accuracy (ACC): 0.7901453627243568\",\n",
            "  \"numTruePositives\": 1499.0,\n",
            "  \"numTrueNegatives\": 10079.0,\n",
            "  \"numPositives\": 3474.0,\n",
            "  \"numNegatives\": 11179.0,\n",
            "  \"description\": \"Computed as (true positive count + true negative count)/(positive_count + negative_count).\",\n",
            "  \"ideal\": \"The ideal value of this metric is 1.0\"\n",
            "}\n",
            "\u001b[1;30m\n",
            "\n",
            "################\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30m\n",
            "\n",
            "Disparate_Impact\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30mNondebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Disparate Impact\",\n",
            "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.0\",\n",
            "  \"numPositivePredictionsUnprivileged\": 531.0,\n",
            "  \"numUnprivileged\": 4857.0,\n",
            "  \"numPositivePredictionsPrivileged\": 2943.0,\n",
            "  \"numPrivileged\": 9796.0,\n",
            "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
            "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30mDebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Disparate Impact\",\n",
            "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.587130327771687\",\n",
            "  \"numPositivePredictionsUnprivileged\": 531.0,\n",
            "  \"numUnprivileged\": 4857.0,\n",
            "  \"numPositivePredictionsPrivileged\": 2943.0,\n",
            "  \"numPrivileged\": 9796.0,\n",
            "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
            "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30m\n",
            "\n",
            "################\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30m\n",
            "\n",
            "Equal_opportunity\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30mNondebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"True Positive Rate Difference\",\n",
            "  \"message\": \"True positive rate difference (true positive rate on unprivileged instances - true positive rate on privileged instances): -0.48487937478763166\",\n",
            "  \"numTruePositivesUnprivileged\": 0.0,\n",
            "  \"numPositivesUnprivileged\": 531.0,\n",
            "  \"numTruePositivesPrivileged\": 1427.0,\n",
            "  \"numPositivesPrivileged\": 2943.0,\n",
            "  \"description\": \"This metric is computed as the difference of true positive rates between the unprivileged and the privileged groups.  The true positive rate is the ratio of true positives to the total number of actual positives for a given group.\",\n",
            "  \"ideal\": \"The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30mDebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"True Positive Rate Difference\",\n",
            "  \"message\": \"True positive rate difference (true positive rate on unprivileged instances - true positive rate on privileged instances): -0.05362336368400744\",\n",
            "  \"numTruePositivesUnprivileged\": 205.0,\n",
            "  \"numPositivesUnprivileged\": 531.0,\n",
            "  \"numTruePositivesPrivileged\": 1294.0,\n",
            "  \"numPositivesPrivileged\": 2943.0,\n",
            "  \"description\": \"This metric is computed as the difference of true positive rates between the unprivileged and the privileged groups.  The true positive rate is the ratio of true positives to the total number of actual positives for a given group.\",\n",
            "  \"ideal\": \"The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30m\n",
            "\n",
            "################\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30m\n",
            "\n",
            "Average_odds\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30mNondebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Average Odds Difference\",\n",
            "  \"message\": \"Average odds difference (average of TPR difference and FPR difference, 0 = equality of odds): -0.30511296916822117\",\n",
            "  \"numFalsePositivesUnprivileged\": 0.0,\n",
            "  \"numNegativesUnprivileged\": 4326.0,\n",
            "  \"numTruePositivesUnprivileged\": 0.0,\n",
            "  \"numPositivesUnprivileged\": 531.0,\n",
            "  \"numFalsePositivesPrivileged\": 859.0,\n",
            "  \"numNegativesPrivileged\": 6853.0,\n",
            "  \"numTruePositivesPrivileged\": 1427.0,\n",
            "  \"numPositivesPrivileged\": 2943.0,\n",
            "  \"description\": \"Computed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups.\",\n",
            "  \"ideal\": \"The ideal value of this metric is 0.  A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30mDebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Average Odds Difference\",\n",
            "  \"message\": \"Average odds difference (average of TPR difference and FPR difference, 0 = equality of odds): -0.03523439799352973\",\n",
            "  \"numFalsePositivesUnprivileged\": 381.0,\n",
            "  \"numNegativesUnprivileged\": 4326.0,\n",
            "  \"numTruePositivesUnprivileged\": 205.0,\n",
            "  \"numPositivesUnprivileged\": 531.0,\n",
            "  \"numFalsePositivesPrivileged\": 719.0,\n",
            "  \"numNegativesPrivileged\": 6853.0,\n",
            "  \"numTruePositivesPrivileged\": 1294.0,\n",
            "  \"numPositivesPrivileged\": 2943.0,\n",
            "  \"description\": \"Computed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups.\",\n",
            "  \"ideal\": \"The ideal value of this metric is 0.  A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.\"\n",
            "}\n",
            "\u001b[1;30m\n",
            "\n",
            "################\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30m\n",
            "\n",
            "Theil_index\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[1;30mNondebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Theil Index\",\n",
            "  \"message\": \"Theil index (generalized entropy index with alpha = 1): 0.1729900486204719\",\n",
            "  \"description\": \"Computed as the generalized entropy of benefit for all individuals in the dataset, with alpha = 1. It measures the inequality in benefit allocation for individuals.\",\n",
            "  \"ideal\": \"A value of 0 implies perfect fairness.\"\n",
            "}\n",
            "\u001b[1;30mDebiasing\u001b[0m\n",
            "{\n",
            "  \"metric\": \"Theil Index\",\n",
            "  \"message\": \"Theil index (generalized entropy index with alpha = 1): 0.17225014337803812\",\n",
            "  \"description\": \"Computed as the generalized entropy of benefit for all individuals in the dataset, with alpha = 1. It measures the inequality in benefit allocation for individuals.\",\n",
            "  \"ideal\": \"A value of 0 implies perfect fairness.\"\n",
            "}\n",
            "\u001b[1;30m\n",
            "\n",
            "################\n",
            "\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH2vW4q8PVN-",
        "colab_type": "text"
      },
      "source": [
        "Now let's collect the `ideal` description for each metric in our previous table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E9ozs_-ME5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "e6b13946-e962-43a3-be18-ef31177af354"
      },
      "source": [
        "acc_ideal = get_ideal_value(ex_debias_test.accuracy())\n",
        "bal_acc_ideal = acc_ideal \n",
        "di_ideal = get_ideal_value(ex_debias_test.disparate_impact())\n",
        "eq_op_ideal = get_ideal_value(ex_debias_test.equal_opportunity_difference())\n",
        "avg_odds_ideal = get_ideal_value(ex_debias_test.average_odds_difference())\n",
        "theil_ideal = get_ideal_value(ex_nondebias_test.theil_index())\n",
        "\n",
        "metrics_final = [[\"Accuracy\", acc_train_non_debias, acc_train_debias, acc_ideal],\n",
        "                [\"Balanced classification accuracy\", bal_acc_non_debias, bal_acc_debias, bal_acc_ideal],\n",
        "                [\"Disparate impact\", di_non_debias, di_debias, di_ideal],\n",
        "                [\"Equal opportunity difference\", eq_op_non_debias, eq_op_debias, eq_op_ideal],\n",
        "                [\"Average odds difference\",avg_odds_non_debias , avg_odds_debias, avg_odds_ideal],\n",
        "                [\"Theil_index\", theil_non_debias, theil_debias, theil_ideal]]\n",
        "headers_final = [\"Classification metric\", \"Without debiasing\",\"With debiasing\", \"Ideal value\"]\n",
        "caption_final = \"Difference in model performance by using Adversarial Learning mitigation\"\n",
        "\n",
        "print_table(headers_final, metrics_final, caption_final)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\"><caption>Difference in model performance by using Adversarial Learning mitigation</caption><tr><th>Classification metric</th><th>Without debiasing</th><th>With debiasing</th><th>Ideal value</th></tr><tr><td>Accuracy</td><td>0.8017</td><td>0.7901</td><td>The ideal value of this metric is 1.0</td></tr><tr><td>Balanced classification accuracy</td><td>0.6670</td><td>0.6665</td><td>The ideal value of this metric is 1.0</td></tr><tr><td>Disparate impact</td><td>0.0000</td><td>0.5871</td><td>The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.</td></tr><tr><td>Equal opportunity difference</td><td>-0.4849</td><td>-0.0536</td><td>The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.</td></tr><tr><td>Average odds difference</td><td>-0.3051</td><td>-0.0352</td><td>The ideal value of this metric is 0.  A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group.</td></tr><tr><td>Theil_index</td><td>0.1730</td><td>0.1723</td><td>A value of 0 implies perfect fairness.</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrykTk1AVUH2",
        "colab_type": "text"
      },
      "source": [
        "# Excercises and questions\n",
        "\n",
        "Let's make sure you understand what you just did while working on this notebook.\n",
        "\n",
        "1. Rerun this notebook with `race` as the protected attribute. How different are the results on the fairness metrics?\n",
        "2. What does the `Adversarial Debiasing` technique do?\n",
        "3. What kind of classifier is this technique using? What hyperparameters could you tune?\n",
        "4. Can I use the current implementation to optimize for several protected attributes?\n"
      ]
    }
  ]
}